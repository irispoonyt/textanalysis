{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irispoonyt/textanalysis/blob/main/product_process_22Nov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srB0NRn_H39B",
        "outputId": "6da8d682-0844-4267-9b15-e51479453681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'darknet'\n",
            "/content\n",
            "sed: can't read Makefile: No such file or directory\n",
            "sed: can't read Makefile: No such file or directory\n",
            "sed: can't read Makefile: No such file or directory\n",
            "sed: can't read Makefile: No such file or directory\n",
            "Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "18 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopencv-dev is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "%cd darknet\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
        "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile\n",
        "!apt update\n",
        "!apt-get install libopencv-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5smSKNXuXjB"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import re \n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import copy\n",
        "import collections\n",
        "import io\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "#gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "\n",
        "#spacy\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN55IdlgudgC"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import WordPunctTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQIxdSCHufiB"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYKtBIq6uiuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c20ae6-b10d-4a09-e4b2-17b049800d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyLDAvis==2.1.2 in /usr/local/lib/python3.8/dist-packages (2.1.2)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis==2.1.2) (0.38.4)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis==2.1.2) (1.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pyLDAvis==2.1.2) (0.16.0)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis==2.1.2) (1.3.5)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis==2.1.2) (2.11.3)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.8/dist-packages (from pyLDAvis==2.1.2) (1.17)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from pyLDAvis==2.1.2) (3.6.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.8/dist-packages (from pyLDAvis==2.1.2) (2.8.4)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis==2.1.2) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis==2.1.2) (1.7.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis==2.1.2) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->pyLDAvis==2.1.2) (22.1.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->pyLDAvis==2.1.2) (9.0.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->pyLDAvis==2.1.2) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pytest->pyLDAvis==2.1.2) (57.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->pyLDAvis==2.1.2) (1.4.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->pyLDAvis==2.1.2) (0.7.1)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import tokenize \n",
        "from operator import itemgetter\n",
        "import math \n",
        "\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "# NLTK Resource\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger') # for tagging\n",
        "nltk.download('stopwords') # stopwords\n",
        "\n",
        "#vis \n",
        "!pip install pyLDAvis==2.1.2\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8fEp7kNum81",
        "outputId": "a4964dcd-c47b-4fc2-b20c-e8ffc5e9ca45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCgfdZn9uocl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "outputId": "dd1b3a92-35ce-4acb-b17b-282de9118bd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        appln_id                     subapplications  \\\n",
              "0       21675509  Physiological parameter monitoring   \n",
              "1       15593310                   Genetics/genomics   \n",
              "2       13443450                   Genetics/genomics   \n",
              "3         855830                    Medical sciences   \n",
              "4       49722486          Neuroscience/neurorobotics   \n",
              "...          ...                                 ...   \n",
              "75096  554839453                      Bioinformatics   \n",
              "75097  554839485                       Public health   \n",
              "75098  554843213                 Medical informatics   \n",
              "75099  554844654          Neuroscience/neurorobotics   \n",
              "75100  554846180                 Medical informatics   \n",
              "\n",
              "                    applications  docdb_family_id appln_auth  \\\n",
              "0      Life and medical sciences          1725797         GB   \n",
              "1      Life and medical sciences          3497097         WO   \n",
              "2      Life and medical sciences          3516369         DE   \n",
              "3      Life and medical sciences          3688388         WO   \n",
              "4      Life and medical sciences          3697785         US   \n",
              "...                          ...              ...        ...   \n",
              "75096  Life and medical sciences         76991704         WO   \n",
              "75097  Life and medical sciences         76991711         WO   \n",
              "75098  Life and medical sciences         76992646         WO   \n",
              "75099  Life and medical sciences         76992923         WO   \n",
              "75100  Life and medical sciences         76993271         WO   \n",
              "\n",
              "      appln_filing_date  appln_filing_year  docdb_family_size  \\\n",
              "0            31/12/9999               9999                  1   \n",
              "1            30/03/2000               2000                  2   \n",
              "2            21/09/1996               1996                  2   \n",
              "3            08/10/2002               2002                  3   \n",
              "4            06/01/1982               1982                  1   \n",
              "...                 ...                ...                ...   \n",
              "75096        22/01/2021               2021                  1   \n",
              "75097        22/01/2021               2021                  1   \n",
              "75098        22/01/2021               2021                  1   \n",
              "75099        19/01/2021               2021                  1   \n",
              "75100        18/10/2020               2020                  1   \n",
              "\n",
              "      appln_nr_original                                     appln_abstract  \\\n",
              "0                   NaN  132,307. Barry, A. G., and Proudfoot, A. Aug. ...   \n",
              "1             EP0002784  The invention relates to a method for prognosi...   \n",
              "2            19638853.0  The method for formulating a model based on th...   \n",
              "3             AT0200289  The invention relates to a device and a method...   \n",
              "4                337397  An evoked response audiometer is disclosed in ...   \n",
              "...                 ...                                                ...   \n",
              "75096     US2021/014684  The present invention provides phase separatio...   \n",
              "75097     US2021/014755  The present disclosure provides methods for co...   \n",
              "75098     US2021/014754  Disclosed herein are systems, devices, and met...   \n",
              "75099     US2021/014003  Provided herein are magnetic resonance imaging...   \n",
              "75100     US2020/056219  A medical device assembly is provided for remo...   \n",
              "\n",
              "      language  \n",
              "0           en  \n",
              "1           en  \n",
              "2           en  \n",
              "3           en  \n",
              "4           en  \n",
              "...        ...  \n",
              "75096       en  \n",
              "75097       en  \n",
              "75098       en  \n",
              "75099       en  \n",
              "75100       en  \n",
              "\n",
              "[75101 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-58dab9b5-f5dd-48ea-b39f-8d7919335970\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>appln_id</th>\n",
              "      <th>subapplications</th>\n",
              "      <th>applications</th>\n",
              "      <th>docdb_family_id</th>\n",
              "      <th>appln_auth</th>\n",
              "      <th>appln_filing_date</th>\n",
              "      <th>appln_filing_year</th>\n",
              "      <th>docdb_family_size</th>\n",
              "      <th>appln_nr_original</th>\n",
              "      <th>appln_abstract</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>21675509</td>\n",
              "      <td>Physiological parameter monitoring</td>\n",
              "      <td>Life and medical sciences</td>\n",
              "      <td>1725797</td>\n",
              "      <td>GB</td>\n",
              "      <td>31/12/9999</td>\n",
              "      <td>9999</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>132,307. Barry, A. G., and Proudfoot, A. Aug. ...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15593310</td>\n",
              "      <td>Genetics/genomics</td>\n",
              "      <td>Life and medical sciences</td>\n",
              "      <td>3497097</td>\n",
              "      <td>WO</td>\n",
              "      <td>30/03/2000</td>\n",
              "      <td>2000</td>\n",
              "      <td>2</td>\n",
              "      <td>EP0002784</td>\n",
              "      <td>The invention relates to a method for prognosi...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13443450</td>\n",
              "      <td>Genetics/genomics</td>\n",
              "      <td>Life and medical sciences</td>\n",
              "      <td>3516369</td>\n",
              "      <td>DE</td>\n",
              "      <td>21/09/1996</td>\n",
              "      <td>1996</td>\n",
              "      <td>2</td>\n",
              "      <td>19638853.0</td>\n",
              "      <td>The method for formulating a model based on th...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>855830</td>\n",
              "      <td>Medical sciences</td>\n",
              "      <td>Life and medical sciences</td>\n",
              "      <td>3688388</td>\n",
              "      <td>WO</td>\n",
              "      <td>08/10/2002</td>\n",
              "      <td>2002</td>\n",
              "      <td>3</td>\n",
              "      <td>AT0200289</td>\n",
              "      <td>The invention relates to a device and a method...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>49722486</td>\n",
              "      <td>Neuroscience/neurorobotics</td>\n",
              "      <td>Life and medical sciences</td>\n",
              "      <td>3697785</td>\n",
              "      <td>US</td>\n",
              "      <td>06/01/1982</td>\n",
              "      <td>1982</td>\n",
              "      <td>1</td>\n",
              "      <td>337397</td>\n",
              "      <td>An evoked response audiometer is disclosed in ...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75096</th>\n",
              "      <td>554839453</td>\n",
              "      <td>Bioinformatics</td>\n",
              "      <td>Life and medical sciences</td>\n",
              "      <td>76991704</td>\n",
              "      <td>WO</td>\n",
              "      <td>22/01/2021</td>\n",
              "      <td>2021</td>\n",
              "      <td>1</td>\n",
              "      <td>US2021/014684</td>\n",
              "      <td>The present invention provides phase separatio...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75097</th>\n",
              "      <td>554839485</td>\n",
              "      <td>Public health</td>\n",
              "      <td>Life and medical sciences</td>\n",
              "      <td>76991711</td>\n",
              "      <td>WO</td>\n",
              "      <td>22/01/2021</td>\n",
              "      <td>2021</td>\n",
              "      <td>1</td>\n",
              "      <td>US2021/014755</td>\n",
              "      <td>The present disclosure provides methods for co...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75098</th>\n",
              "      <td>554843213</td>\n",
              "      <td>Medical informatics</td>\n",
              "      <td>Life and medical sciences</td>\n",
              "      <td>76992646</td>\n",
              "      <td>WO</td>\n",
              "      <td>22/01/2021</td>\n",
              "      <td>2021</td>\n",
              "      <td>1</td>\n",
              "      <td>US2021/014754</td>\n",
              "      <td>Disclosed herein are systems, devices, and met...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75099</th>\n",
              "      <td>554844654</td>\n",
              "      <td>Neuroscience/neurorobotics</td>\n",
              "      <td>Life and medical sciences</td>\n",
              "      <td>76992923</td>\n",
              "      <td>WO</td>\n",
              "      <td>19/01/2021</td>\n",
              "      <td>2021</td>\n",
              "      <td>1</td>\n",
              "      <td>US2021/014003</td>\n",
              "      <td>Provided herein are magnetic resonance imaging...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75100</th>\n",
              "      <td>554846180</td>\n",
              "      <td>Medical informatics</td>\n",
              "      <td>Life and medical sciences</td>\n",
              "      <td>76993271</td>\n",
              "      <td>WO</td>\n",
              "      <td>18/10/2020</td>\n",
              "      <td>2020</td>\n",
              "      <td>1</td>\n",
              "      <td>US2020/056219</td>\n",
              "      <td>A medical device assembly is provided for remo...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75101 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58dab9b5-f5dd-48ea-b39f-8d7919335970')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-58dab9b5-f5dd-48ea-b39f-8d7919335970 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-58dab9b5-f5dd-48ea-b39f-8d7919335970');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ],
      "source": [
        "df1 = pd.read_csv(\"/content/drive/MyDrive/AIdata/onlyfamily_id_en.csv\", sep = ';',header=0, decimal=\",\")\n",
        "# df1 = pd.read_csv(\"/content/drive/MyDrive/Iris/AIdata/onlyfamily_id_en.csv\", sep = ';',header=0, decimal=\",\")\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRELg1IluqIg"
      },
      "outputs": [],
      "source": [
        "# selected column headers we want to use \n",
        "new_headers = ['appln_id', 'appln_abstract']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fs1aqanUureP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "f4c4fba5-bca4-4dd1-9da0-ea140b6d4bdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        appln_id                                     appln_abstract\n",
              "0       21675509  132,307. Barry, A. G., and Proudfoot, A. Aug. ...\n",
              "1       15593310  The invention relates to a method for prognosi...\n",
              "2       13443450  The method for formulating a model based on th...\n",
              "3         855830  The invention relates to a device and a method...\n",
              "4       49722486  An evoked response audiometer is disclosed in ...\n",
              "...          ...                                                ...\n",
              "75096  554839453  The present invention provides phase separatio...\n",
              "75097  554839485  The present disclosure provides methods for co...\n",
              "75098  554843213  Disclosed herein are systems, devices, and met...\n",
              "75099  554844654  Provided herein are magnetic resonance imaging...\n",
              "75100  554846180  A medical device assembly is provided for remo...\n",
              "\n",
              "[75101 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aaca7d3b-3fd5-4a45-bbc4-264af170cde7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>appln_id</th>\n",
              "      <th>appln_abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>21675509</td>\n",
              "      <td>132,307. Barry, A. G., and Proudfoot, A. Aug. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15593310</td>\n",
              "      <td>The invention relates to a method for prognosi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13443450</td>\n",
              "      <td>The method for formulating a model based on th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>855830</td>\n",
              "      <td>The invention relates to a device and a method...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>49722486</td>\n",
              "      <td>An evoked response audiometer is disclosed in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75096</th>\n",
              "      <td>554839453</td>\n",
              "      <td>The present invention provides phase separatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75097</th>\n",
              "      <td>554839485</td>\n",
              "      <td>The present disclosure provides methods for co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75098</th>\n",
              "      <td>554843213</td>\n",
              "      <td>Disclosed herein are systems, devices, and met...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75099</th>\n",
              "      <td>554844654</td>\n",
              "      <td>Provided herein are magnetic resonance imaging...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75100</th>\n",
              "      <td>554846180</td>\n",
              "      <td>A medical device assembly is provided for remo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75101 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aaca7d3b-3fd5-4a45-bbc4-264af170cde7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aaca7d3b-3fd5-4a45-bbc4-264af170cde7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aaca7d3b-3fd5-4a45-bbc4-264af170cde7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ],
      "source": [
        "# remove unwanted columns keep selected headers\n",
        "abstract = pd.DataFrame(df1, columns=new_headers)\n",
        "\n",
        "abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaHJU-PhutdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eb9740d-cda8-422b-f5d8-27db41462c3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "appln_id           int64\n",
              "appln_abstract    object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ],
      "source": [
        "# check data type\n",
        "abstract.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBj8lyRUuvPt"
      },
      "outputs": [],
      "source": [
        "# word count are min 1 to max 175 \n",
        "\n",
        "# Identify most common words\n",
        "common = pd.Series(' '.join(abstract['appln_abstract']).split()).value_counts()[:40]\n",
        "common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x96X1PGbuxA2"
      },
      "outputs": [],
      "source": [
        "#Identify uncommon words\n",
        "uncommon =  pd.Series(' '.join(abstract\n",
        "         ['appln_abstract']).split()).value_counts()[-40:]\n",
        "uncommon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgibUL53xGXv"
      },
      "outputs": [],
      "source": [
        "#Cleaning the data\n",
        "# convert to lowercase\n",
        "abstract['appln_abstract'] = abstract['appln_abstract'].str.lower()\n",
        "\n",
        "# convert desc to string\n",
        "abstract['appln_abstract'] = abstract['appln_abstract'].astype(str)\n",
        "\n",
        "abstract.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in0mbS4vYNKg"
      },
      "source": [
        "build list A and list B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQl5xSysYPDP"
      },
      "outputs": [],
      "source": [
        "# Defining a list of dictionaries in Python \n",
        "list_process = ['process','method','algorithm', 'processing','detection','analysis','control','monitoring','provides', 'step', \n",
        "                    'generating', 'evaluating', 'optimal','efficient','acquisition','identification','calculating','receive','server','characteristic','subsequent','support','framework','point','sensor','watch','screen','mode','screening','extraction','procedure','workflow','automatic','continous','assembling','building','compose','construct',\n",
        "                    'cast','coat','cut','fabricating','form','install','making','manufacture','mold','prepare','shape','spray','decompose',\n",
        "                    'breed','model','training-sample','delivery','organise','organize','sort','corresponding-system','corresponding-program','corresponding-stage',\n",
        "                'configure','system']\n",
        "  \n",
        "dictOfprocess = { i : 1 for i in list_process}\n",
        "#dictOfprocess = dict.fromkeys(list_process , 1)\n",
        "\n",
        "# Printing the results\n",
        "print(dictOfprocess)\n",
        " \n",
        "# Validating the type of 'dictOfprocess' and its element\n",
        "print(type(dictOfprocess))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1A8DB36enrT"
      },
      "outputs": [],
      "source": [
        "# Defining a list of dictionaries in Python \n",
        "list_product = ['product','connected','feature','function','device','equipment','effective','terminal','type','attribute','multi','multifunctional',\n",
        "                'realize','accuracy','replacement','adjustment','instrument','tool', 'additional','broaden','extend','broaden-the-range',\n",
        "                'medium','platform','frame','application','specificity','circuit','discovery','results','various','apparatus','solution','means',\n",
        "                 ]\n",
        "  \n",
        "dictOfproduct = { i : 1 for i in list_product}\n",
        "#dictOfprocess = dict.fromkeys(list_process , 1)\n",
        "\n",
        "# Printing the results\n",
        "print(dictOfproduct)\n",
        " \n",
        "# Validating the type of 'dictOfprocess' and its element\n",
        "print(type(dictOfproduct))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1WzOwpgxZWS"
      },
      "outputs": [],
      "source": [
        "#creating the stop word list and adding to it\n",
        "# library for text preprocessing\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')   # Required for tokenization\n",
        "nltk.download('wordnet') # Required for lemmatization\n",
        "\n",
        "#nltk.download('wordnet') \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gnVUu1qxkrC"
      },
      "outputs": [],
      "source": [
        "# create a list of stop words and adding custom stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# create a list of custom stopwords\n",
        "new_words =  ['The','a', 'of', 'and', 'to', 'is', 'the', 'in', 'for', 'an', 'on', 'image', 'with', 'data', 'by', 'are', 'be', \n",
        "              'or', 'can', 'that', 'invention', 'based', 'medical', 'from', 'as', 'at', 'information', 'one', \n",
        "              'comprises','first', 'which', 'device', 'through', 'each', 'using', 'used', 'according', 'sciences', 'data', 'according', 'patient', 'time', 'network',\n",
        "              'using','body', 'user', 'used', 'unit', 'discloses', 'second', 'set', 'provided that','provided with','images', 'signal', \n",
        "              'plurality', 'least', 'value', 'us', 'target', 'includes','real','de','result','corresponding','diagnosis','may','obtain','parameter',\n",
        "              'may','obtain','utility','region','determining','part','field','obtained','present','treatment','neural','training',\n",
        "              'high','parameters', 'disease','wherein','sensor','computer','end','area','improved','arranged','learning','structure',\n",
        "              'dimensional','obtaining','input','state','sample','display','operation','genetics','genomics','optimization','output',\n",
        "              'storage','following','power','subject','including','test','condition','position','physiological','performing','different',\n",
        "              'segmentation','human','side','surface','prediction','heart','point','object','use','plate','cell','pressure','signals','comprise','problem','values',\n",
        "              'tissue','informatics','box','rate','sequence','associated','layer','number','technical','selected','nursing','measurement','intelligent','level',\n",
        "              'current','individual','temperature','reference','brain','frequency','said','energy', 'whether', 'clinical','jp','digital',\n",
        "              'light','face','low','electronic','cancer', 'within', 'vessel', 'risk','pixel', 'upper','biological', 'ct', 'fixed',\n",
        "              'population','medicine','respectively','protein','degree','threshold','p','pulse','embodiment','2011','original','water',\n",
        "              'person','28', 'and', 'as','(ãâ¢ãâãâpetãâ¢ãâãâ)','g'\n",
        "]\n",
        "stop_words = stop_words.union(new_words)\n",
        "\n",
        "stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNvHMomqX78D"
      },
      "outputs": [],
      "source": [
        "#remove useless words/ stopwords\n",
        "text1 = abstract['appln_abstract'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))\n",
        "\n",
        "text1.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR9DYUdSGeod"
      },
      "outputs": [],
      "source": [
        "text1=text1.astype(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOhAjAy7wfOm"
      },
      "outputs": [],
      "source": [
        "# Instantiate\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIVpynv1wjUK"
      },
      "outputs": [],
      "source": [
        "# Exclusion list of punctuations and numbers\n",
        "exclist = string.punctuation + string.digits\n",
        "# Print the exclusion list\n",
        "print(exclist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDXComTZwm4p"
      },
      "source": [
        "First, we will tokenize first before removing numbers and punctuation, so that the hypenated words can be kept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE4p_agtw04l"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Create a Function\n",
        "def clean_texts(text1):\n",
        "    \"\"\" Function to perform preprocessing \"\"\"\n",
        "\n",
        "    \n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text1)\n",
        "        \n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "\n",
        "    # Join tokens\n",
        "    clean_text = \" \".join(tokens)\n",
        "\n",
        "   \n",
        "   # Remove punctuations and numbers\n",
        "    text1 = text1.translate(str.maketrans(\"\", \"\", exclist))\n",
        "  \n",
        "    # Return the output\n",
        "    return text1\n",
        "# Apply the function to all disclosures\n",
        "text1 = text1.apply(clean_texts)\n",
        "# View the first 5 rows\n",
        "text1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmMMGO_E0Dgk"
      },
      "outputs": [],
      "source": [
        "print(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s02JSROBOEsH"
      },
      "outputs": [],
      "source": [
        "print(abstract)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmHqlASbw8pL"
      },
      "outputs": [],
      "source": [
        "!pip install rake-nltk -q\n",
        "!pip install python-rake==1.4.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHGOc4wBzuKz"
      },
      "outputs": [],
      "source": [
        "from rake_nltk import Rake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_gP4MSEs3jI"
      },
      "outputs": [],
      "source": [
        "import RAKE\n",
        "import operator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(stop_words))"
      ],
      "metadata": {
        "id": "y0s26LUtHhPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2-zwJH6s99I"
      },
      "outputs": [],
      "source": [
        "stop_words1=list(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(stop_words))"
      ],
      "metadata": {
        "id": "92Nx9IEeGvhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnopCgIlu5us"
      },
      "outputs": [],
      "source": [
        "rake_object=RAKE.Rake(stop_words1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbHUib1wu_p4"
      },
      "outputs": [],
      "source": [
        "def Sort_Tuple(tup):\n",
        "  #reverse=None(Sorts in Ascending order)\n",
        "  #key is set to sort using second element of\n",
        "  #sublist lambda has been used\n",
        "  tup.sort(key=lambda x: x[1])\n",
        "  return tup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-T6H91exkU0"
      },
      "outputs": [],
      "source": [
        "#convert object to string\n",
        "text2=str(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtHUgoAS1oLO"
      },
      "outputs": [],
      "source": [
        "r = Rake(punctuations = [')','(',',',':','),',').','.'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1Vdx51GvPWA"
      },
      "outputs": [],
      "source": [
        "r=Rake()\n",
        "r.extract_keywords_from_text(text2)\n",
        "r.get_ranked_phrases()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twtZyDYszz4r"
      },
      "outputs": [],
      "source": [
        "#double-checking\n",
        "# word_score = degree(word)/frequency(word)r.extract_keywords_from_text(text[100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_PwuhrfAlkP"
      },
      "outputs": [],
      "source": [
        "keywords=r.extract_keywords_from_text(text1[100])\n",
        "extracted_keyword=r.get_ranked_phrases()\n",
        "print(extracted_keyword)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N6Gr8OLcWCU"
      },
      "outputs": [],
      "source": [
        "r.get_ranked_phrases()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psUuU0ctvSQd"
      },
      "outputs": [],
      "source": [
        "len(extracted_keyword)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLaFaPVW51Ya"
      },
      "outputs": [],
      "source": [
        "# word_score = degree(word)/frequency(word)\n",
        "phrase_df = pd.DataFrame(r.get_ranked_phrases_with_scores(), columns = ['score','phrase'])\n",
        "phrase_df.loc[phrase_df.score>3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CQfA02Y-uyI"
      },
      "source": [
        "Keywords extraction (total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDLG_OvNwxJR"
      },
      "outputs": [],
      "source": [
        "# Python3 code to demonstrate working of\n",
        "# Extract Keywords from String List\n",
        " \n",
        "# Using iskeyword() + loop + split()\n",
        "import keyword\n",
        " \n",
        " # Create a Function\n",
        "def clean_texts(text1):\n",
        "    \"\"\" Function to perform preprocessing \"\"\"\n",
        "\n",
        "    \n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text1)\n",
        "        \n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "\n",
        "    # Join tokens\n",
        "    clean_text = \" \".join(tokens)\n",
        "\n",
        "   # Remove punctuations and numbers\n",
        "    text1 = text1.translate(str.maketrans(\"\", \"\", exclist))\n",
        "    \n",
        "    # Return the output\n",
        "    return clean_text\n",
        "# Apply the function to all disclosures\n",
        "text1 = text1.apply(clean_texts)\n",
        "\n",
        "# iterating using loop\n",
        "res = []\n",
        "for sub in text1:\n",
        "   for word in sub.split():\n",
        " \n",
        "       # check for keyword using iskeyword()\n",
        "       if keyword.iskeyword(word):\n",
        "           res.append(word)\n",
        "\n",
        "patentkw=str(res)\n",
        " \n",
        "# printing result\n",
        "print(\"Extracted Keywords : \" + patentkw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYGrQkO0w6aH"
      },
      "outputs": [],
      "source": [
        "len(patentkw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ0TB7vSP5cH"
      },
      "outputs": [],
      "source": [
        "dictOfpatent = { i : 1 for i in patentkw}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xxRZyazBjUc"
      },
      "outputs": [],
      "source": [
        "abstract.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV-Ir8LO0Akg"
      },
      "outputs": [],
      "source": [
        "#on a macro-level, which keyword is more commonly found is list A and list B, respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_3dou7UYp4d"
      },
      "outputs": [],
      "source": [
        "#count number of items in the list/ length of a list\n",
        "#printing result\n",
        "length_process=len(dictOfprocess)\n",
        "length_product=len(dictOfproduct)\n",
        "print(\"The length of process list is :\"+str(length_process))\n",
        "print(\"The length of product list is :\"+str(length_product))\n",
        "\n",
        "\n",
        "#length of process=64; whereas length of product=34\n",
        "#Since list A and list B are of different lengths, so I need to calculate each pair of keywords in list A and list B, \n",
        "#calculate the occurrence by counting or mutual information, Then I will get a size of list A x size of list B\n",
        "#a model with measure of tendency is necessary to run regression on the i-th patent\n",
        "#average over list PMI (keywords in patent abstract, keywords in list A and list B)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71qPF3AmEwHC"
      },
      "outputs": [],
      "source": [
        "#for each row of abstract keywords, build a cooccurence matrix with List A and List B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x99_oB0rtGk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "\n",
        "def by_indexes(iterable):\n",
        "    output = {}\n",
        "    for index, key in enumerate(iterable):\n",
        "        output.setdefault(key, []).append(index)\n",
        "    return output\n",
        "\n",
        "\n",
        "def co_occurrence_matrix(text1, dictOfprocess, window_size=5):\n",
        "    def split_tokens(tokens):\n",
        "        for token in tokens:\n",
        "            indexs = dictOfprocess_indexes.get(token)\n",
        "            if indexs is not None:\n",
        "                yield token, indexs[0]\n",
        "\n",
        "    matrix1 = np.zeros((len(dictOfprocess), len(dictOfprocess)), np.float64)\n",
        "    dictOfprocess_indexes = by_indexes(dictOfprocess)\n",
        "\n",
        "    for sent in text1:\n",
        "        tokens = by_indexes(split_tokens(sent.split())).items()\n",
        "        for ((word_1, x), indexes_1), ((word_2, y), indexes_2) in itertools.permutations(tokens, 2):\n",
        "            for k in indexes_1:\n",
        "                for l in indexes_2:\n",
        "                    if abs(l - k) <= window_size:\n",
        "                        matrix1[x, y] += 1\n",
        "    return matrix1\n",
        "\n",
        "process_matrix=co_occurrence_matrix(text1, dictOfprocess)\n",
        "\n",
        "print(process_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9zhe5amzlvP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "\n",
        "def by_indexes(iterable):\n",
        "    output = {}\n",
        "    for index, key in enumerate(iterable):\n",
        "        output.setdefault(key, []).append(index)\n",
        "    return output\n",
        "\n",
        "\n",
        "def co_occurrence_matrix(text1, dictOfproduct, window_size=5):\n",
        "    def split_tokens(tokens):\n",
        "        for token in tokens:\n",
        "            indexs = dictOfproduct_indexes.get(token)\n",
        "            if indexs is not None:\n",
        "                yield token, indexs[0]\n",
        "\n",
        "    matrix2 = np.zeros((len(dictOfproduct), len(dictOfproduct)), np.float64)\n",
        "    dictOfproduct_indexes = by_indexes(dictOfproduct)\n",
        "\n",
        "    for sent in text1:\n",
        "        tokens = by_indexes(split_tokens(sent.split())).items()\n",
        "        for ((word_1, x), indexes_1), ((word_2, y), indexes_2) in itertools.permutations(tokens, 2):\n",
        "            for k in indexes_1:\n",
        "                for l in indexes_2:\n",
        "                    if abs(l - k) <= window_size:\n",
        "                        matrix2[x, y] += 1\n",
        "    return matrix2\n",
        "\n",
        "product_matrix=co_occurrence_matrix(text1, dictOfproduct)\n",
        "\n",
        "print(product_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-63L9IHvRq0"
      },
      "outputs": [],
      "source": [
        "plt.imshow(process_matrix,interpolation='nearest', cmap='Reds')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIMa6BN5vFVa"
      },
      "outputs": [],
      "source": [
        "plt.imshow(product_matrix,interpolation='nearest', cmap='Reds')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkm8DjNfxqs1"
      },
      "source": [
        "The graph suggests that the patent is more product-oriented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1uCCDoq5iqN"
      },
      "outputs": [],
      "source": [
        "patentkw1=list(patentkw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-qyxAaHJTbC"
      },
      "outputs": [],
      "source": [
        "#co-ocurrence matrix: study overall correlation\n",
        "#mutual information between an abstract and a list of dictionary\n",
        "#gives you some information about closeness in the vector space\n",
        "#1. calculate which keyword is most found among all abstracts, list A and list B\n",
        " \n",
        "# Most common keyword in abstract and list process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WekIzmy-gZp"
      },
      "outputs": [],
      "source": [
        "#2. Which one appears more on list A but not list B (or the other way round)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAmiyWBWyyJB"
      },
      "outputs": [],
      "source": [
        "#Part 1, both occures\n",
        "#Part 2: occur list A but not list B\n",
        "#Part 3: occur list B but not list ]\\\n",
        "#Part 4: both not found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs4kVlt4ZZ6c"
      },
      "source": [
        "Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1_-3HIxTlOy"
      },
      "outputs": [],
      "source": [
        "#check the electronyque machine - case study\n",
        "!pip3 install keybert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "famcNYgCbvUf"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "example= \"\"\"A seizure detection system including one or more circuits, the one or more circuits configured to receive an electroencephalogram (EEG) signal generated based on electrical brain activity of a patient. The one or more circuits are configured to determine metrics based on the EEG signal, the metrics indicating non-linear features of the EEG signal, determine that the EEG signal indicates a candidate seizure by determining, based at least in part on the metrics, a change in the non-linear features of the EEG signal over time, and generate a seizure alert indicating that the EEG signal indicates the candidate seizure. The change in the non-linear features indicates a physiological force that gives rise to the candidate seizure.\"\"\"\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "keywords3 = kw_model.extract_keywords(example)\n",
        "print(keywords3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aas7mtHdAytC"
      },
      "outputs": [],
      "source": [
        "len(keywords3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X66nXA8F5LL"
      },
      "outputs": [],
      "source": [
        "coocur_matrix = np.zeros((len(keywords3), len(list_process)),np.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzc3X3ojrxOZ"
      },
      "source": [
        "25 Nov calculate PMI for each patent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLOu8ZO1aArG"
      },
      "outputs": [],
      "source": [
        "text3=pd.DataFrame(text1) #<---data frame version\n",
        "print(text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTkTfwqYambi"
      },
      "outputs": [],
      "source": [
        "#Convert the dictionary into DataFrame\n",
        "dfprocess=pd.Series(dictOfprocess)\n",
        "dfprocess=pd.Series(dictOfproduct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCRkh7HwcM__"
      },
      "outputs": [],
      "source": [
        "print(dfprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmWmzDCrdlzw"
      },
      "outputs": [],
      "source": [
        "print(pmi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss6rgHXGffeN"
      },
      "outputs": [],
      "source": [
        "ListA = \"respiratory analysis system ( 101 ) monitoring system\"\n",
        "#dictOfA=dict(ListA,False)\n",
        "dfA=pd.Series(ListA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtgKwszlhZQf"
      },
      "outputs": [],
      "source": [
        "print (dfA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGsr3dwT8hZX"
      },
      "outputs": [],
      "source": [
        "#practise PMI score 28 Nov 2022\n",
        "\n",
        "from glob import glob\n",
        "import nltk\n",
        "from nltk import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "\n",
        "from keybert import KeyBERT\n",
        "import itertools\n",
        "import numpy as np\n",
        "np.random.seed(3) # for reproducibility\n",
        "\n",
        "\n",
        "# for tokenization\n",
        "import spacy\n",
        "\n",
        "# for log2\n",
        "import math\n",
        "from math import log\n",
        "\n",
        "# for bigrams\n",
        "from nltk import bigrams\n",
        "\n",
        " # Create a Function\n",
        "def clean_texts(dfA):\n",
        "    \"\"\" Function to perform preprocessing \"\"\"\n",
        "#convert it to a token\n",
        "\n",
        "    tokens = nltk.word_tokenize(dfA)\n",
        "        \n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    dfA= dfA.translate(str.maketrans(\"\", \"\", exclist))\n",
        "    \n",
        "\n",
        "    # Join tokens\n",
        "    clean_text = \" \".join(tokens)\n",
        "\n",
        "\n",
        "    return dfA\n",
        "    \n",
        "dfA1=dfA.apply(clean_texts)\n",
        "\n",
        "import re\n",
        "dfA2=dfA1.to_string(index=False)\n",
        "keywordfA = re.findall(r'\\w+',dfA2) #it is a list\n",
        "print(keywordfA)\n",
        "\n",
        "#kw_model = KeyBERT()\n",
        "#keywordfA = kw_model.extract_keywords(dfA1)\n",
        "\n",
        "# printing result\n",
        "print(\"Extracted Keywords : \" + str(keywordfA)) #<------extraction of keywords keywordfA= keywords for patent\n",
        "\n",
        "\n",
        "# initializing dict to store frequency of each element\n",
        "#for patent keyword\n",
        "elements_count = {}\n",
        "# iterating over the elements for frequency\n",
        "for element in keywordfA:\n",
        "   # checking whether it is in the dict or not\n",
        "   if element in elements_count:\n",
        "      # incerementing the count by 1\n",
        "      elements_count[element] += 1\n",
        "   else:\n",
        "      # setting the count to 1\n",
        "      elements_count[element] = 1\n",
        "# printing the elements frequencies\n",
        "for key, value in elements_count.items():\n",
        "   print(f\"{key}: {value}\")\n",
        "\n",
        "   \n",
        "   countdfA=elements_count.items()\n",
        "\n",
        "countprocess=dictOfprocess.items()\n",
        "\n",
        "print(countprocess)\n",
        "#make a list of unique items in keyword list\n",
        "unique_valuesA=set(keywordfA) #avoid duplications\n",
        "\n",
        "# generate combinations for each sub list seperately  \n",
        "list=[unique_valuesA, list_process] #avoid duplications \n",
        "\n",
        "combination = [p for p in itertools.product(*list)]  #compute bigrams for keywords in patent and process list\n",
        "\n",
        "combination_diagonal = np.diagonal(combination) \n",
        "\n",
        "#total lengths of the list/ total items of the list\n",
        "\n",
        "total_combination=len(combination)\n",
        "\n",
        "totalA=len(keywordfA)\n",
        "\n",
        "length_process=length_process\n",
        "\n",
        "\n",
        "\n",
        "#prob of word in abstract\n",
        "for item in keywordfA:\n",
        "    c = Counter(keywordfA)\n",
        "    totalA = sum(c.values())\n",
        "    percentA = {key: value/totalA for key, value in c.items()}\n",
        "#    percentA = {key: value/totalA for key, value in c.items()}\n",
        "    print(percentA)\n",
        "\n",
        "    # convert to list\n",
        "    percent_listA = [percentA.get(str(i), 0.0) for i in range(5)]\n",
        "    #    percent_listA = [percentA.get(str(i), 0.0) for i in range(5)]\n",
        "    print(percent_listA)\n",
        "\n",
        "\n",
        "for item in list_process:\n",
        "    d = Counter(list_process)\n",
        "    totalprocess = sum(d.values())\n",
        "    percentprocess = {key: value/totalprocess for key, value in d.items()}\n",
        "    print(percentprocess)\n",
        "\n",
        "    # convert to list\n",
        "    percent_listprocess = [percentprocess.get(str(i), 0.0) for i in range(5)]\n",
        "    print(percent_listprocess)\n",
        "\n",
        "#calculate probabilities of co-occurence\n",
        "for item in combination:\n",
        "    e = Counter(combination)\n",
        "    totalcomb = sum(e.values())\n",
        "    percent_cooccurence = {key: value/totalcomb for key, value in e.items()}\n",
        "    print(percent_cooccurence)\n",
        "\n",
        "    # convert to list\n",
        "    percent_listcooccurence = [percent_cooccurence.get(str(i), 0.0) for i in range(5)]\n",
        "    print(percent_listcooccurence)\n",
        "\n",
        "#calculate pmi\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for (f,g,h) in zip(percentA, percentprocess,percent_cooccurence):\n",
        "    f = Counter(percentA)\n",
        "    g = Counter(percentprocess)\n",
        "    h = Counter(percent_cooccurence)\n",
        "\n",
        "def calculate_pmi(cooccurrence, prob_word1, prob_word2):\n",
        "    \"\"\"\n",
        "    Calcualtes the pmi of 2 words\n",
        "    \"\"\"\n",
        "    \n",
        "    return math.log2(cooccurrence / (prob_word1 * prob_word2))\n",
        "\n",
        "\n",
        "# A list storing all the pmi values\n",
        "all_pmi = []\n",
        "\n",
        "\n",
        "for k, v in h.items():\n",
        "    # k = tuple, e.g. ('respiratoty, control')\n",
        "    # v = prob of cooccurrence, e.g. 0.15625\n",
        "    \n",
        "    # Extract the two words from the tuple\n",
        "    word1 = k[0]\n",
        "    word2 = k[1]\n",
        "    #print(word1, word2)\n",
        "    \n",
        "    # Get the probability of the word in f and g\n",
        "    prob_word1 = f.get(word1)\n",
        "    prob_word2 = g.get(word2)\n",
        "    \n",
        " \n",
        "    pmi = calculate_pmi(v, prob_word1, prob_word2)\n",
        "    all_pmi.append(pmi)\n",
        "    print(f\"Word 1: {word1}, Word 2: {word2}, prob(Word 1 in f): {prob_word1}, prob(Word 2 in g): {prob_word2}, pmi = {pmi}\")\n",
        "    \n",
        "    \n",
        "# Calculate the mean of all the pmi values\n",
        "pmi_mean = sum(all_pmi) / len(all_pmi)\n",
        "print(f\"Mean of all the pmi values = {pmi_mean}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwknh1FKYDKT"
      },
      "outputs": [],
      "source": [
        "print(f\"Mean of all the pmi values = {pmi_mean}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1LevUqL5-xy"
      },
      "outputs": [],
      "source": [
        "#Loop for every column (every patent abstract) + process\n",
        "#1 Dec 2022\n",
        "from glob import glob\n",
        "import nltk\n",
        "from nltk import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "\n",
        "from keybert import KeyBERT\n",
        "import itertools\n",
        "import numpy as np\n",
        "np.random.seed(3) # for reproducibility\n",
        "import re\n",
        "\n",
        "\n",
        "# for tokenization\n",
        "import spacy\n",
        "\n",
        "# for log2\n",
        "import math\n",
        "from math import log\n",
        "\n",
        "# for bigrams\n",
        "from nltk import bigrams\n",
        "\n",
        "# Create a Function\n",
        "\n",
        "def clean_texts(text1):\n",
        "  \n",
        "  tokens2 = nltk.word_tokenize(text1)   #convert it to a token\n",
        "  \n",
        "  tokens2 = [lemmatizer.lemmatize(token) for token in tokens2] # Lemmatization\n",
        "  \n",
        "  text1= text1.translate(str.maketrans(\"\", \"\", exclist))# Remove punctuations and numbers\n",
        "  \n",
        "  clean_text = \" \".join(tokens2)# Join tokens\n",
        "  \n",
        "  return text1\n",
        "\n",
        "\n",
        "for ind in text1:\n",
        "\n",
        "  text4 = text1.apply(clean_texts(ind))\n",
        "# text5=text4.to_string(index=False)\n",
        "\n",
        "\n",
        "  keyword_pt = re.findall(r'\\W+',text4) #it is a list\n",
        "#print(keyword_pt)\n",
        "\n",
        "'''\n",
        "#break here\n",
        "#kw_model = KeyBERT()\n",
        "#keywordfA = kw_model.extract_keywords(dfA1)\n",
        "\n",
        "# printing result\n",
        "print(\"Extracted Keywords : \" + str(keyword_pt)) #<------extraction of keywords keywordfA= keywords for patent\n",
        "\n",
        "\n",
        "# initializing dict to store frequency of each element\n",
        "#for patent keyword\n",
        "elements_count = {}\n",
        "# iterating over the elements for frequency\n",
        "for element in keyword_pt:\n",
        "   # checking whether it is in the dict or not\n",
        "   if element in elements_count:\n",
        "      # incerementing the count by 1\n",
        "      elements_count[element] += 1\n",
        "   else:\n",
        "      # setting the count to 1\n",
        "      elements_count[element] = 1\n",
        "# printing the elements frequencies\n",
        "for key, value in elements_count.items():\n",
        "  elements=f\"{key}:{value}\"\n",
        "  #print(f\"{key}: {value}\")\n",
        "  countpt=elements_count.items()\n",
        "  \n",
        "  countprocess=dictOfprocess.items()\n",
        "\n",
        "print(countprocess)\n",
        "#make a list of unique items in keyword list\n",
        "unique_valuespt=set(keyword_pt) #avoid duplications\n",
        "\n",
        "# generate combinations for each sub list seperately  \n",
        "list1=[unique_valuespt, list_process] #avoid duplications \n",
        "\n",
        "combination_process = [p for p in itertools.product(*list1)]  #compute bigrams for keywords in patent and process list\n",
        "\n",
        "combination_diagonalx = np.diagonal(combination_process) \n",
        "\n",
        "#total lengths of the list/ total items of the list\n",
        "\n",
        "total_combinationx=len(combination_process)\n",
        "\n",
        "totalpt=len(keyword_pt)\n",
        "\n",
        "length_process=length_process\n",
        "\n",
        "\n",
        "\n",
        "#prob of word in abstract\n",
        "for item in keyword_pt:\n",
        "    c = Counter(keyword_pt)\n",
        "    totalpt = sum(c.values())\n",
        "    percentpt = {key: value/totalpt for key, value in c.items()}\n",
        "\n",
        "#   print(percentpt)\n",
        "\n",
        "    # convert to list\n",
        "    percent_listpt = [percentpt.get(str(i), 0.0) for i in range(5)]\n",
        "\n",
        "#    print(percent_listA)\n",
        "\n",
        "\n",
        "for item in list_process:\n",
        "    d = Counter(list_process)\n",
        "    totalprocess = sum(d.values())\n",
        "    percentprocess = {key: value/totalprocess for key, value in d.items()}\n",
        "    print(percentprocess)\n",
        "\n",
        "    # convert to list\n",
        "    percent_listprocess = [percentprocess.get(str(i), 0.0) for i in range(5)]\n",
        "    print(percent_listprocess)\n",
        "\n",
        "#calculate probabilities of co-occurence\n",
        "for item in combination_process:\n",
        "    e = Counter(combination_process)\n",
        "    total_combx = sum(e.values())\n",
        "    percent_cooccurence = {key: value/totalcombx for key, value in e.items()}\n",
        "    print(percent_cooccurence)\n",
        "\n",
        "    # convert to list\n",
        "    percent_listcooccurence = [percent_cooccurence.get(str(i), 0.0) for i in range(5)]\n",
        "    print(percent_listcooccurence)\n",
        "\n",
        "#calculate pmi\n",
        "\n",
        "\n",
        "\n",
        "for (f,g,h) in zip(percentpt, percentprocess,percent_cooccurence):\n",
        "    f = Counter(percentpt)\n",
        "    g = Counter(percentprocess)\n",
        "    h = Counter(percent_cooccurence)\n",
        "\n",
        "def calculate_pmi(cooccurrence, prob_word1, prob_word2):\n",
        "    \"\"\"\n",
        "    Calcualtes the pmi of 2 words\n",
        "    \"\"\"\n",
        "    \n",
        "    return math.log2(cooccurrence / (prob_word1 * prob_word2))\n",
        "\n",
        "\n",
        "# A list storing all the pmi values\n",
        "all_pmi = []\n",
        "\n",
        "\n",
        "for k, v in h.items():\n",
        "    # k = tuple, e.g. ('respiratoty, control')\n",
        "    # v = prob of cooccurrence, e.g. 0.15625\n",
        "    \n",
        "    # Extract the two words from the tuple\n",
        "    word1 = k[0]\n",
        "    word2 = k[1]\n",
        "    #print(word1, word2)\n",
        "    \n",
        "    # Get the probability of the word in f and g\n",
        "    prob_word1 = f.get(word1)\n",
        "    prob_word2 = g.get(word2)\n",
        "    \n",
        " \n",
        "    pmi = calculate_pmi(v, prob_word1, prob_word2)\n",
        "    all_pmi.append(pmi)\n",
        "    print(f\"Word 1: {word1}, Word 2: {word2}, prob(Word 1 in f): {prob_word1}, prob(Word 2 in g): {prob_word2}, pmi = {pmi}\")\n",
        "    \n",
        "    \n",
        "# Calculate the mean of all the pmi values\n",
        "pmi_mean = sum(all_pmi) / len(all_pmi)\n",
        "print(f\"Mean of all the pmi values = {pmi_mean}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_texts(text1):\n",
        "  \n",
        "  tokens2 = nltk.word_tokenize(text1)   #convert it to a token\n",
        "  \n",
        "  tokens2 = [lemmatizer.lemmatize(token) for token in tokens2] # Lemmatization\n",
        "  \n",
        "  text1= text1.translate(str.maketrans(\"\", \"\", exclist))# Remove punctuations and numbers\n",
        "  \n",
        "  clean_text = \" \".join(tokens2)# Join tokens\n",
        "  \n",
        "  return text1\n",
        "\n",
        "#text4=text3.to_string(index=False)\n",
        "\n",
        "for ind in text1:\n",
        "  print(clean_texts(ind))"
      ],
      "metadata": {
        "id": "vXhSXGKmdFCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print"
      ],
      "metadata": {
        "id": "ZodAgLGDXD9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Create a Function\n",
        "\n",
        "def clean_texts(text1):\n",
        "  \n",
        "  tokens = nltk.word_tokenize(text1) # Join tokens\n",
        "  \n",
        "  tokens = [lemmatizer.lemmatize(token) for token in tokens] # Lemmatization\n",
        "  \n",
        "  text1= text1.translate(str.maketrans(\"\", \"\", exclist))    # Remove punctuations and numbers\n",
        "  \n",
        "  clean_text = \" \".join(tokens)# Join tokens\n",
        "  \n",
        "  return text1\n",
        "\n",
        "text4=text1.astype(str)\n",
        "#text4=text1.to_string(index=False)\n",
        "\n",
        "for row in text1.T.iteritems():\n",
        "  \n",
        "  text4=text1.apply(clean_texts(row))\n",
        "  \n",
        "  keyword_pt= re.findall(r'\\w+',text4) #it is a list\n",
        "'''  print(keyword_pt)'''"
      ],
      "metadata": {
        "id": "XadSzDj6Sh1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)"
      ],
      "metadata": {
        "id": "0Fxf5D4mXdYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(keyword_pt[0:30])"
      ],
      "metadata": {
        "id": "gQ7LP6DQ-6Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO2QiNcLH2aQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW4zvZe1Mbk0"
      },
      "outputs": [],
      "source": [
        "#Loop for every column (every patent abstract) + product\n",
        "'''\n",
        "from glob import glob\n",
        "import nltk\n",
        "from nltk import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "\n",
        "from keybert import KeyBERT\n",
        "import itertools\n",
        "import numpy as np\n",
        "np.random.seed(3) # for reproducibility\n",
        "\n",
        "\n",
        "# for tokenization\n",
        "import spacy\n",
        "\n",
        "# for log2\n",
        "import math\n",
        "from math import log\n",
        "\n",
        "# for bigrams\n",
        "from nltk import bigrams\n",
        "\n",
        " # Create a Function\n",
        "\n",
        "for ind in text3.index:\n",
        "  \n",
        "  def clean_texts(text3):\n",
        "    \"\"\" Function to perform preprocessing \"\"\"\n",
        "#convert it to a token\n",
        "\n",
        "    tokens = nltk.word_tokenize(text3)\n",
        "        \n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    text3= text3.translate(str.maketrans(\"\", \"\", exclist))\n",
        "    \n",
        "\n",
        "    # Join tokens\n",
        "    clean_text = \" \".join(tokens)\n",
        "\n",
        "\n",
        "    return text3\n",
        "\n",
        "    \n",
        "text4 = text3.apply(clean_texts)\n",
        "\n",
        "#import re\n",
        "#text41=text4.to_string(index=False)\n",
        "#keyword_pt = re.findall(r'\\w+',text41) #it is a list\n",
        "#print(keyword_pt)\n",
        "\n",
        "#kw_model = KeyBERT()\n",
        "#keywordfA = kw_model.extract_keywords(dfA1)\n",
        "\n",
        "# printing result\n",
        "#print(\"Extracted Keywords : \" + str(keyword_pt)) #<------extraction of keywords keywordfA= keywords for patent\n",
        "\n",
        "\n",
        "# initializing dict to store frequency of each element\n",
        "#for patent keyword\n",
        "elements_count = {}\n",
        "# iterating over the elements for frequency\n",
        "for element in keyword_pt:\n",
        "   # checking whether it is in the dict or not\n",
        "   if element in elements_count:\n",
        "      # incerementing the count by 1\n",
        "      elements_count[element] += 1\n",
        "   else:\n",
        "      # setting the count to 1\n",
        "      elements_count[element] = 1\n",
        "# printing the elements frequencies\n",
        "for key, value in elements_count.items():\n",
        "   print(f\"{key}: {value}\")\n",
        "\n",
        "   \n",
        "   countpt=elements_count.items()'''\n",
        "\n",
        "countproduct=dictOfproduct.items()\n",
        "\n",
        "print(countproduct)\n",
        "#make a list of unique items in keyword list\n",
        "unique_valuespt=set(keyword_pt) #avoid duplications\n",
        "\n",
        "# generate combinations for each sub list seperately  \n",
        "list2=[unique_valuespt, list_product] #avoid duplications \n",
        "\n",
        "combination_product = [p for p in itertools.product(*list2)]  #compute bigrams for keywords in patent and process list\n",
        "\n",
        "combination_diagonaly = np.diagonal(combination_product) \n",
        "\n",
        "#total lengths of the list/ total items of the list\n",
        "\n",
        "total_combinationy=len(combination_product)\n",
        "\n",
        "totalpt=len(keyword_pt)\n",
        "\n",
        "length_product=length_product\n",
        "\n",
        "\n",
        "\n",
        "#prob of word in abstract\n",
        "for item in keyword_pt:\n",
        "    c = Counter(keyword_pt)\n",
        "    totalpt = sum(c.values())\n",
        "    percentpt = {key: value/totalpt for key, value in c.items()}\n",
        "\n",
        "#   print(percentpt)\n",
        "\n",
        "    # convert to list\n",
        "    percent_listpt = [percentpt.get(str(i), 0.0) for i in range(5)]\n",
        "\n",
        "#    print(percent_listA)\n",
        "\n",
        "\n",
        "for item in list_product:\n",
        "    d = Counter(list_product)\n",
        "    totalproduct = sum(d.values())\n",
        "    percentproduct = {key: value/totalproduct for key, value in d.items()}\n",
        "    print(percentproduct)\n",
        "\n",
        "    # convert to list\n",
        "    percent_listproduct = [percentproduct.get(str(i), 0.0) for i in range(5)]\n",
        "    print(percent_listproduct)\n",
        "\n",
        "#calculate probabilities of co-occurence\n",
        "for item in combination_product:\n",
        "    e = Counter(combination_product)\n",
        "    totalcomby = sum(e.values())\n",
        "    percent_cooccurence = {key: value/totalcomby for key, value in e.items()}\n",
        "    print(percent_cooccurence)\n",
        "\n",
        "    # convert to list\n",
        "    percent_listcooccurence = [percent_cooccurence.get(str(i), 0.0) for i in range(5)]\n",
        "    print(percent_listcooccurence)\n",
        "\n",
        "#calculate pmi\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for (f,g,h) in zip(percentpt, percentprocess,percent_cooccurence):\n",
        "    f = Counter(percentpt)\n",
        "    g = Counter(percentprocess)\n",
        "    h = Counter(percent_cooccurence)\n",
        "\n",
        "def calculate_pmi(cooccurrence, prob_word1, prob_word2):\n",
        "    \"\"\"\n",
        "    Calcualtes the pmi of 2 words\n",
        "    \"\"\"\n",
        "    \n",
        "    return math.log2(cooccurrence / (prob_word1 * prob_word2))\n",
        "\n",
        "\n",
        "for k, v in h.items():\n",
        "    # k = tuple, e.g. ('respiratoty, control')\n",
        "    # v = prob of cooccurrence, e.g. 0.15625\n",
        "    \n",
        "    # Extract the two words from the tuple\n",
        "    word1 = k[0]\n",
        "    word2 = k[1]\n",
        "    #print(word1, word2)\n",
        "    \n",
        "    # Get the probability of the wor in f and g\n",
        "    prob_word1 = f.get(word1)\n",
        "    prob_word2 = g.get(word2)\n",
        "    \n",
        " \n",
        "    pmi_product = calculate_pmi(v, prob_word1, prob_word2)\n",
        "    print(f\"Word 1: {word1}, Word 2: {word2}, prob(Word 1 in f): {prob_word1}, prob(Word 2 in g): {prob_word2}, pmi = {pmi_product}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-BTavfwYQSX"
      },
      "outputs": [],
      "source": [
        "print(type(text3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHwp4s_eI5E1"
      },
      "outputs": [],
      "source": [
        "print(text3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HZf5F0qYDcF"
      },
      "outputs": [],
      "source": [
        "  def clean_texts(text3):\n",
        "    \"\"\" Function to perform preprocessing \"\"\"\n",
        "#convert it to a token\n",
        "\n",
        "    tokens = nltk.word_tokenize(text3)\n",
        "        \n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    text3= text3.translate(str.maketrans(\"\", \"\", exclist))\n",
        "    \n",
        "\n",
        "    # Join tokens\n",
        "    clean_text = \" \".join(tokens)\n",
        "\n",
        "\n",
        "    return text3\n",
        "\n",
        "    \n",
        "text4=text3.apply(clean_texts)\n",
        "\n",
        "import re\n",
        "text41=text4.to_string(index=False)\n",
        "keyword_pt = re.findall(r'\\w+',text41) #it is a list\n",
        "print(keyword_pt[30])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ56-hSSYb1q"
      },
      "outputs": [],
      "source": [
        "sns.set_theme()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "\n",
        "sns.scatterplot(data=df, x='pmi_process', y='pmi_product', ax=ax)\n",
        "\n",
        "plt.title('product and process matrix')\n",
        "\n",
        "plt.xlabel('Product')\n",
        "\n",
        "plt.ylabel('Process')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUamTNd0A51y"
      },
      "outputs": [],
      "source": [
        "len(combination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai3pP3TyyQH4"
      },
      "outputs": [],
      "source": [
        "# generate combinations for each sub list seperately\n",
        "lists_of_pairs = [list(itertools.combinations(sub_list, 2)) for sub_list in names]\n",
        "# flatten the lists of pairs to 1 large list of pairs\n",
        "all_pairs = [pair for pairs_list in lists_of_pairs for pair in pairs_list]\n",
        "# let the Counter do the rest for you\n",
        "co_occurences_counts = Counter(all_pairs)\n",
        "\n",
        "\n",
        "#compute co-occurence\n",
        "def listOccurences(item, names):\n",
        "    # item is the list that you want to check, eg. ['cat','fish']\n",
        "    # names contain the list of list you have.\n",
        "    set_of_items = set(item) # set(['cat','fish'])\n",
        "    count = 0\n",
        "    for value in names:\n",
        "        if set_of_items & set(value) == set_of_items:\n",
        "            count+=1\n",
        "    return count\n",
        "\n",
        "names =  [['cat', 'fish'], ['cat'], ['fish', 'dog', 'cat'],['cat', 'bird', 'fish'], ['fish', 'bird']]\n",
        "# Now for each of your possibilities which you can generate\n",
        "# Chain flattens the list, set removes duplicates, and combinations generates all possible pairs.\n",
        "permuted_values = list(itertools.combinations(set(itertools.chain.from_iterable(names)), 2))\n",
        "d = {}\n",
        "for v in permuted_values:\n",
        "    d[str(v)] = listOccurences(v, names)\n",
        "# The key in the dict being a list cannot be possible unless it's converted to a string.\n",
        "print(d)\n",
        "# {\"['fish', 'dog']\": 1, \"['cat', 'dog']\": 1, \"['cat', 'fish']\": 3, \"['cat', 'bird']\": 1, \"['fish', 'bird']\": 2}\n",
        "\n",
        "\n",
        "\n",
        "#create co-occurence matrix\n",
        "\n",
        "\n",
        "#compute PMI score\n",
        "\n",
        "\n",
        "def pmi(df):\n",
        "    '''\n",
        "    Calculate the positive pointwise mutal information score for each entry\n",
        "    https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
        "    We use the log( p(y|x)/p(y) ), y being the column, x being the row\n",
        "    '''\n",
        "    # Get numpy array from pandas df\n",
        "    dfA2 = dfA1.to_numpy()\n",
        "    dfprocess1=dfprocess.to_numpy()\n",
        "\n",
        " \n",
        "    # Get numpy array from pandas text1 and list process\n",
        "    arr = text1.as_matrix()\n",
        "    brr=list_process.as_matrix()\n",
        "\n",
        "\n",
        "    # p(y|x) probability of each t1 overlap within the row\n",
        "    row_totals = arr.sum(axis=1).astype(float)\n",
        "    prob_cols_given_row = (arr.T / row_totals).T\n",
        "\n",
        "    # p(y) probability of each t1 in the total set\n",
        "    col_totals = arr.sum(axis=0).astype(float)\n",
        "    prob_of_cols = col_totals / sum(col_totals)\n",
        "\n",
        "    # PMI: log( p(y|x) / p(y) )\n",
        "    # This is the same data, normalized\n",
        "    ratio = prob_cols_given_row / prob_of_cols\n",
        "    ratio[ratio==0] = 0.00001\n",
        "    _pmi = np.log(ratio)\n",
        "    _pmi[_pmi < 0] = 0\n",
        "\n",
        "    return _pmi\n",
        "    return _pmi\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}